# Example: Swarm (SN124)
# Autonomous drone flight control via RL policy competition
# Pattern: container_execution with simulation-based evaluation

metadata:
  name: swarm
  subnet: 124
  repo: "https://github.com/swarm-subnet/swarm"
  category: robotics
  patterns_used:
    - container_execution  # validators run miner policies
  simulation: "PyBullet"
  domain: "Autonomous drone navigation"

# ===========================================================================
# WHAT IS BEING MEASURED
# ===========================================================================

measurement:
  one_sentence: "Drone autopilot policy quality on navigation tasks"
  commodity: software  # → container_execution pattern
  
  detail: |
    Miners submit pre-trained RL policies for drone control.
    Validators generate secret flight tasks (MapTasks).
    Policies evaluated in PyBullet simulation on success + speed.
    Winner-take-all: top performer gets 25% of emissions.

  mission: "Develop open-source autonomous drone autopilot"

# ===========================================================================
# MECHANISM OVERVIEW
# ===========================================================================

mechanism:
  summary: |
    Validators generate random MapTasks (start→goal flight missions).
    Miners submit pre-trained RL policies without knowing tasks.
    Validators run policies in PyBullet simulator.
    Score = 50% success (reach + hover) + 50% time efficiency.
    Top performer wins all emissions for that round.

  flow:
    validator:
      1_generate_task: "Create MapTask with random goal (10-30m away)"
      2_keep_secret: "Tasks are hidden from miners"
      3_load_policies: "Load miner-submitted policies"
      4_simulate: "Run policies headless in PyBullet"
      5_measure: "Track success, hover time, flight time"
      6_score: "Calculate reward: success + time"
      7_select_winner: "Identify top performer"
      8_set_weights: "Winner gets emissions, others get nothing"
    
    miner:
      1_train: "Train RL policy on drone navigation"
      2_export: "Export model (PyTorch, SB3, ONNX)"
      3_submit: "Provide PolicyRef with metadata"
      4_wait: "Policy evaluated on secret tasks"
      5_compete: "Hope to be top performer"

# ===========================================================================
# MAP TASK SPECIFICATION
# ===========================================================================

map_task:
  description: "Random flight mission from start to goal"
  
  parameters:
    goal_distance:
      min: 10  # meters
      max: 30  # meters
      sampling: "Radial from start"
    
    altitude: "Random variation"
    
    seed: "Reproducible randomness"
    
    simulation:
      physics: "PyBullet"
      time_step: "sim_dt parameter"
      horizon: "Maximum time limit"
  
  secret: |
    Tasks are generated by validators and NOT disclosed to miners.
    Policies must generalize to unseen start/goal configurations.

# ===========================================================================
# POLICY REQUIREMENTS
# ===========================================================================

policy:
  format:
    supported_frameworks:
      - pytorch: "state_dict or full model"
      - stable_baselines3: "SB3 policy module"
      - onnx: "Exported ONNX format"
      - torchscript: "JIT traced model"
  
  submission:
    policy_ref:
      sha256: "Hash of model file"
      framework: "pytorch | stable_baselines3 | onnx"
      size_bytes: "Model file size"
    
    model_file: "policy.pt or equivalent"
    
    inference_code: "Optional: script to run policy"
  
  output:
    type: "FlightPlan"
    format: "Open-loop rotor speed commands"
    structure: "Sequence of [time, rotor1, rotor2, rotor3, rotor4]"
  
  constraints:
    deterministic: "Must produce same output for same input"
    self_contained: "Include all preprocessing/normalization"
    eval_mode: "No randomness during evaluation"

# ===========================================================================
# REWARD FUNCTION
# ===========================================================================

reward:
  formula: "score = 0.5 * success + 0.5 * time_efficiency"
  
  components:
    success:
      weight: 0.5
      definition: "Reach goal AND hover for required duration"
      hover_time: "3-5 seconds depending on version"
      binary: "1.0 if successful, 0.0 if failed"
    
    time_efficiency:
      weight: 0.5
      formula: "1 - (elapsed_time / horizon)"
      interpretation: "Faster completion = higher score"
      normalization: "Relative to max allowed time"
  
  range: "[0, 1]"
  
  alternative_weights:
    note: "Some sources suggest 70/15/15 split"
    success: 70%
    time: 15%
    energy: 15%

# ===========================================================================
# WINNER-TAKE-ALL DISTRIBUTION
# ===========================================================================

emission_distribution:
  mechanism: winner_take_all
  
  structure:
    winner: "25% of round emissions"
    others: "0% - nothing for non-winners"
  
  rationale: |
    Creates strong incentive for best-in-class performance.
    Miners must consistently be top performer to earn.
  
  consequence:
    high_variance: "Feast or famine for miners"
    innovation_pressure: "Must continuously improve"

# ===========================================================================
# SIMULATION ENVIRONMENT
# ===========================================================================

simulation:
  engine: "PyBullet"
  
  drone_model:
    type: "Micro quadrotor"
    rotors: 4
    control: "Rotor speed commands"
  
  physics:
    realistic: "Full dynamics simulation"
    time_step: "Configurable sim_dt"
    deterministic: "Same seed = same physics"
  
  evaluation:
    headless: "No visualization during eval"
    reproducible: "Seeded randomness"
    isolated: "Each policy runs independently"

# ===========================================================================
# ANTI-GAMING PROPERTIES
# ===========================================================================

anti_gaming:
  secret_tasks:
    mechanism: "MapTasks generated by validators, hidden from miners"
    benefit: "Cannot memorize or overfit to specific tasks"
  
  random_goals:
    mechanism: "Goal positions randomized (10-30m, varying altitude)"
    benefit: "Requires generalization, not lookup tables"
  
  deterministic_evaluation:
    mechanism: "Policies must be deterministic"
    benefit: "Reproducible results across validators"
  
  winner_take_all:
    mechanism: "Only top performer earns"
    benefit: "No reward for mediocre performance"
  
  open_loop_output:
    mechanism: "Policies output FlightPlan (no feedback during execution)"
    benefit: "Tests robust planning, not reactive hacks"

# ===========================================================================
# COMPARISON TO OTHER SUBNETS
# ===========================================================================

pattern_comparison:
  vs_kinitro:
    similarity: "Both evaluate RL policies in simulation"
    difference: "Swarm is drones, Kinitro is robotic manipulation"
    difference: "Swarm uses winner-take-all, Kinitro uses proportional"
  
  vs_affine:
    similarity: "Both run agent code, evaluate performance"
    difference: "Swarm evaluates in physics sim, Affine in reasoning envs"
  
  vs_container_execution:
    alignment: "Miners compete on policy quality"
    implementation: "Validators run policies in PyBullet sandbox"

# ===========================================================================
# CONNECTIONS TO KNOWLEDGE BASE
# ===========================================================================

knowledge_connections:
  subnet.invariants.yaml:
    validator_only_development: "Miners submit policies, validators evaluate"
    open_source_preference: "Policies can be any framework"
    compute_distribution: "Validators run simulation"
  
  container_execution.yaml:
    pattern_match: "Miners compete on code/policy quality"
    implementation: "PyBullet simulation sandbox"
    evaluation: "Objective success + time metrics"
  
  incentive.primitives.yaml:
    commodity_type: "software (RL policies)"
    verification_method: "direct (simulation evaluation)"
    scoring_formula: "0.5 * success + 0.5 * time"
    anti_gaming: "Secret tasks, determinism, winner-take-all"
  
  design_flow.yaml:
    commodity: software
    ground_truth: "Simulation success signal"
    pattern: container_execution

# ===========================================================================
# LESSONS LEARNED
# ===========================================================================

lessons:
  secret_task_generation:
    insight: |
      Keeping evaluation tasks secret from miners forces
      policies to generalize rather than memorize.
      Random seed makes tasks reproducible for validators.
    applies_to: "Any simulation-based evaluation subnet"
  
  winner_take_all_incentives:
    insight: |
      Winner-take-all creates extreme pressure for excellence
      but high variance for miners. Good for driving innovation,
      but may discourage smaller participants.
    applies_to: "Highly competitive subnets"
  
  open_loop_policy_output:
    insight: |
      Requiring open-loop FlightPlans (no feedback during execution)
      tests robust planning ability, not reactive control.
      Different from closed-loop RL evaluation.
    applies_to: "Planning-focused robotics subnets"
  
  physics_simulation_as_ground_truth:
    insight: |
      Using physics simulation (PyBullet) provides objective,
      reproducible ground truth. All validators see same behavior
      for same policy + task.
    applies_to: "Robotics and control subnets"
  
  multi_framework_support:
    insight: |
      Supporting multiple frameworks (PyTorch, SB3, ONNX)
      lowers barrier for miners with different toolchains.
    applies_to: "ML model submission subnets"

# ===========================================================================
# QUICK REFERENCE
# ===========================================================================

quick_reference:
  measuring: "Drone autopilot policy quality on navigation tasks"
  pattern: "container_execution with PyBullet simulation"
  miner_provides: "Pre-trained RL policy (PyTorch, SB3, ONNX)"
  validator_does: "Generates secret MapTasks, runs policies, scores performance"
  simulation: "PyBullet physics engine"
  scoring: "50% success (reach + hover) + 50% time efficiency"
  distribution: "Winner-take-all (top gets 25%, others get nothing)"
  novel_mechanism: "Secret task generation + open-loop FlightPlan output"
  key_invariant: "Secret tasks force generalization, prevent memorization"
